{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Adapted by [Jen Ferguson](https://library.northeastern.edu/about/library-staff-directory/jen-ferguson) from notebooks created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br /> See [here](https://docs.tdm-pilot.org/tag/intermediate-lessons/) for the originals and additional text analysis notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About JSTOR/Portico and tdm-pilot.org##\n",
    "*What is this thing I searched when creating my dataset?*\n",
    "\n",
    "The text mining platform at tdm-pilot.org provides access to:\n",
    "* nearly all of JSTOR \n",
    "* content from selected Portico publishers (currently 40+ publishers)\n",
    "* content from Chronicling America - historic newspapers\n",
    "* CORD - a COVID-19 collection\n",
    "* DocSouth - primary source collection about the history & culture of the American South\n",
    "\n",
    "Discussions with other providers are underway.\n",
    "\n",
    "By the numbers: All told, the above includes content from 2000+ publishers, 5500+ journals, and 19 million articles. \n",
    "\n",
    "Note: Northeastern has an agreement with JSTOR/Portico, so we have permission to do this.\n",
    "\n",
    "If you generated your own dataset, the slick interface that you used at tdm-pilot.org is mostly intended to give you a peek into what's in your data. For more detailed work, you can pull that dataset you generated into a Jupyter Notebook. That's what we'll do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding significant words within a curated dataset##\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using a model called TF-IDF. \n",
    "\n",
    "*Fun fact: TF-IDF was used in early search engines to do relevance ranking, until clever folks figured out how to break that by 'keyword stuffing'.* \n",
    "\n",
    "As you work through this notebook, you'll take the following steps:\n",
    "\n",
    "* Import a dataset\n",
    "* Find the query used to build the dataset within the dataset's metadata\n",
    "* Write a helper function to help clean up a single token\n",
    "* Clean each document of your dataset, one token at a time\n",
    "* Use a dictionary of English words to remove words with poor OCR (optical character recognition)\n",
    "* Compute the most significant words in your corpus using TF-IDF and a library callled gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's a token?**  It's a string of text. For our purposes, think of a token = a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note before we get started. As you work through this notebook you may see a cell or two marked ***'optional'***. These are opportunities for you to try modifying and applying Python code to see what happens. I encourage you to try them, but you can also just run the notebook as written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import gensim, and the Dataset module from the tdm_client library.  The tdm_client library contains functions for connecting to the JSTOR server that contains our corpus dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "\n",
    "import tdm_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll pull in our datasets. \n",
    "\n",
    "\n",
    "**Did you build your own dataset?**  In the next code cell, you'll supply the dataset ID provided when you created your dataset. Now's a good time to make sure you have it handy.\n",
    "\n",
    "**Didn't create a dataset?**  Here are a couple to choose from, with dataset IDs in <font color=red> red </font>:\n",
    "\n",
    "\n",
    "* Documents published in African American Review, Black American Literature Forum, and Negro American Literature Forum (from JSTOR): <font color=red> b4668c50-a970-c4d7-eb2c-bb6d04313542 </font>\n",
    "\n",
    "\n",
    "* 'Civilian Conservation Corps' from Chronicling America:<font color=red> 9fa82dbc-9269-6deb-9720-179b4ba5e451</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of data derived from searching JSTOR for 'Civilian Conservation Corps' is provided here ('e2a07be0-39f4-4b9f-b3d1-680bb04dc580'). \n",
    "\n",
    "Pasting your unique **dataset ID** in place of the <font color=red> red </font> text below will import your dataset from the JSTOR server. (No output will show.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = '6ef4b79b-73a2-7590-afcd-0b22e64a2a46\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = tdm_client.get_description(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info[\"num_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check to make sure we have the correct dataset. \n",
    "We can look at the original query by viewing the `search_desription`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_info[\"search_description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've verified that we have the correct corpus/dataset, let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = tdm_client.get_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a helper function that can standardize and clean up the tokens in it. The function will:\n",
    "\n",
    "* Change all tokens (aka words) to lower case.  This will make 'Cats' and 'cats' be counted as the same token.\n",
    "* Discard tokens with non-alphabetical characters\n",
    "* Discard any tokens less than 4 characters in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question to ponder:* Why do you think we want to discard tokens that are less than 4 characters long?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #defines a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #changes all strings to lower case\n",
    "    if len(token) < 4: #discards any tokens that are less than 4 characters long\n",
    "        return\n",
    "    if not(token.isalpha()): #discards any tokens with non-alphabetic characters\n",
    "        return\n",
    "    return token #returns the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the corpus with our helper function.  This may take a while to run; recall that if it's in process, you'll see this: In [ * ]. (No output will show.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tdm_client.dataset_reader(dataset_file)\n",
    "\n",
    "#Creates a new variable `documents` that is a list that that will contain all of our documents.\n",
    "documents = []\n",
    "\n",
    "for n, document in enumerate(reader):\n",
    "    this_doc = []\n",
    "    _id = document[\"id\"]\n",
    "    for token, count in document[\"unigramCount\"].items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append((_id, this_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([d[1] for d in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in [d[1] for d in documents]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have those pieces in place, we can run the following code cells to find the most significant terms, by TFIDF, in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, weight in sorted_td[:20]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 20 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(documents[n][0], dictionary.get(word_id), score)\n",
    "    if n >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional:  How would you print the most significant word for the **first 8 documents**? Copy the code block above, and paste then modify the code in the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to keep going? [This notebook](https://hub.binder.tdm-pilot.org/user/jasf--tdm-nbs-dxsy8b0z/notebooks/Count%20and%20Visualize.ipynb/) will let you count and visualize some characteristics of the documents in your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
