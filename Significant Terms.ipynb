{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding significant words within a curated dataset##\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using a model called TF-IDF. \n",
    "\n",
    "*Fun fact: TF-IDF was used in early search engines as a way to do relevance ranking, until clever folks figured out a way to break it with keyword stuffing.* \n",
    "\n",
    "As you work through this notebook, you'll take the following steps:\n",
    "\n",
    "* Import your dataset\n",
    "* Find your initial query within your dataset's metadata\n",
    "* Write a helper function to help clean up a single token\n",
    "* Clean each document of your dataset, one token at a time\n",
    "* Use a dictionary of English words to remove words with poor OCR\n",
    "* Compute the most significant words in your corpus using TFIDF and a library callled gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's a token?**  It's a string of text. For our purposes, think of a token = a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note before we get started. As you work through this notebook you'll see cells marked ***'optional'***. These are opportunities for you to try modifying and applying Python code to see what happens. I encourage you to try them, but you can also just run the notebook as written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import gensim, and the Dataset module from the tdm_client library.  The tdm_client library contains functions for connecting to the JSTOR server containing our corpus dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tdm_client import Dataset\n",
    "from tdm_client import htrc_corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze your dataset, use the dataset ID provided when you created your dataset.\n",
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of data derived from searching JSTOR for 'antibiotic' and 'resistance' and 'coli' is provided here ('730b508b-5152-618a-2856-aa1a2900a0b2'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server. (No output will show.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('730b508b-5152-618a-2856-aa1a2900a0b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check to make sure we have the correct dataset. \n",
    "We can look at the original query by using the query_text method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'antibiotic coli resistance from JSTOR from 1985 - 2020'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've verified that we have the correct corpus/dataset, let's create a helper function that can standardize and clean up the tokens in it. The function will:\n",
    "\n",
    "* Change all tokens (aka words) to lower case.  This will make 'Cats' and 'cats' be counted as the same token.\n",
    "* Use a dictionary from The HathiTrust Research Center to correct common OCR (Optical Character Recognition) problems\n",
    "* Remove stopwords based on an The HathiTrust Research Center stopword list\n",
    "* Discard tokens with non-alphabetical characters\n",
    "* Discard tokens less than 4 characters in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question to ponder:* Why do you think we want to discard tokens that are less than 4 characters long?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #defines a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #changes all strings to lower case\n",
    "    corrected = htrc_corrections.get(token) #this is a function that fixes common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #discards any tokens that are less than 4 characters long\n",
    "        return\n",
    "    if not(token.isalpha()): #discards any tokens with non-alphabetic characters\n",
    "        return\n",
    "    return token #returns the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the corpus with our helper function.  This may take a while to run; recall that if it's in process, you'll see this: In [ * ]. (No output will show.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] #Creates a new variable `documents` that is a list that will contain all of our documents.\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have those pieces in place, we can run the following code cells to find the most significant terms, by TFIDF, in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creg 0.985472072383967\n",
      "mdrgnb 0.9752274211736038\n",
      "biba 0.9710149484549401\n",
      "orndcase 0.9649982520576849\n",
      "letx 0.9643944773788852\n",
      "cidl 0.9586607209180106\n",
      "gapr 0.957340331085051\n",
      "rpar 0.9554485770666841\n",
      "cjfur 0.9521815564827408\n",
      "annatl 0.9506783080042027\n",
      "xylitol 0.9495181584343583\n",
      "whmd 0.9449462729206862\n",
      "rrfhcp 0.9431778937694659\n",
      "rifaximin 0.9410790102054745\n",
      "lemir 0.9378245131200877\n",
      "mcjd 0.9372176832162501\n",
      "squalamine 0.9361024495260135\n",
      "repa 0.9351415405166057\n",
      "sqrr 0.9319239029829915\n",
      "npma 0.9308989171767714\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:20]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 20 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/30085290 imipenem 0.51903961751567\n",
      "http://www.jstor.org/stable/23361671 piperacillin 0.42524608618659304\n",
      "http://www.jstor.org/stable/26159053 dysenteriae 0.6768243695571163\n",
      "http://www.jstor.org/stable/41435597 empirical 0.3587476010614525\n",
      "http://www.jstor.org/stable/23049339 avleal 0.5022447328372467\n",
      "http://www.jstor.org/stable/2354239 multiacetylation 0.3279667356249203\n",
      "http://www.jstor.org/stable/43872707 eaggec 0.32786877806876613\n",
      "http://www.jstor.org/stable/41328436 trgnb 0.47431510044765346\n",
      "http://www.jstor.org/stable/j.ctvb4bssr.20 fructose 0.856928157119235\n",
      "http://www.jstor.org/stable/24590958 eaea 0.5129162634554189\n",
      "http://www.jstor.org/stable/24809490 cpka 0.8303579361928378\n",
      "http://www.jstor.org/stable/3372748 pneumoniae 0.4657366194728223\n",
      "http://www.jstor.org/stable/34198 gsls 0.6185192757750724\n",
      "http://www.jstor.org/stable/10.1086/664768 meat 0.4116906562360587\n",
      "http://www.jstor.org/stable/24077776 waldvogel 0.2974469445257273\n",
      "http://www.jstor.org/stable/4457009 niehs 0.36439169823108847\n",
      "http://www.jstor.org/stable/4536731 pegasys 0.5033105224022942\n",
      "http://www.jstor.org/stable/4484931 teleological 0.803032048365874\n",
      "http://www.jstor.org/stable/23388955 race 0.40271426348744666\n",
      "http://www.jstor.org/stable/4481615 lamblia 0.6610688780362829\n",
      "http://www.jstor.org/stable/23498121 rocephin 0.5283714439795274\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional:  How would you print the most significant word for the **first 8 documents**? Modify the code block above and paste your modified code in the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/30085290 imipenem 0.51903961751567\n",
      "http://www.jstor.org/stable/23361671 piperacillin 0.42524608618659304\n",
      "http://www.jstor.org/stable/26159053 dysenteriae 0.6768243695571163\n",
      "http://www.jstor.org/stable/41435597 empirical 0.3587476010614525\n",
      "http://www.jstor.org/stable/23049339 avleal 0.5022447328372467\n",
      "http://www.jstor.org/stable/2354239 multiacetylation 0.3279667356249203\n",
      "http://www.jstor.org/stable/43872707 eaggec 0.32786877806876613\n",
      "http://www.jstor.org/stable/41328436 trgnb 0.47431510044765346\n",
      "http://www.jstor.org/stable/j.ctvb4bssr.20 fructose 0.856928157119235\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to learn more and/or try setting up your own Jupyter Notebook?   [This is a great tutorial.](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
