{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding significant words within a curated dataset##\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using a model called TF-IDF. \n",
    "\n",
    "*Fun fact: TF-IDF was used in early search engines to do relevance ranking, until clever folks figured out how to break that by 'keyword stuffing'.* \n",
    "\n",
    "As you work through this notebook, you'll take the following steps:\n",
    "\n",
    "* Import your dataset\n",
    "* Find your initial query within your dataset's metadata\n",
    "* Write a helper function to help clean up a single token\n",
    "* Clean each document of your dataset, one token at a time\n",
    "* Use a dictionary of English words to remove words with poor OCR\n",
    "* Compute the most significant words in your corpus using TF-IDF and a library callled gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's a token?**  It's a string of text. For our purposes, think of a token = a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note before we get started. As you work through this notebook you'll see cells marked ***'optional'***. These are opportunities for you to try modifying and applying Python code to see what happens. I encourage you to try them, but you can also just run the notebook as written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import gensim, and the Dataset module from the tdm_client library.  The tdm_client library contains functions for connecting to the JSTOR server containing our corpus dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "\n",
    "import tdm_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll pull in our datasets. To analyze your dataset, you'll use the dataset ID provided when you created your dataset.\n",
    "\n",
    "**Didn't create a dataset?**  Here are a few to choose from:\n",
    "\n",
    "* 'Civilian Conservation Corps' from Chronicling America:<font color=red> 9fa82dbc-9269-6deb-9720-179b4ba5e451</font>\n",
    "\n",
    "\n",
    "* Documents published in African American Review, Black American Literature Forum, and Negro American Literature Forum (from JSTOR): <font color=red> b4668c50-a970-c4d7-eb2c-bb6d04313542 </font>\n",
    "\n",
    "\n",
    "* Documents with keyword 'liaison' from various LIS publications in JSTOR: <font color=red> 59b7e8b6-1e4c-3c5f-3967-ef5722aa2b00 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of data derived from searching JSTOR for 'Civilian Conservation Corps' is provided here ('e2a07be0-39f4-4b9f-b3d1-680bb04dc580'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server. (No output will show.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'e2a07be0-39f4-4b9f-b3d1-680bb04dc580'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = tdm_client.get_description(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info[\"num_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check to make sure we have the correct dataset. \n",
    "We can look at the original query by viewing the `search_desription`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_info[\"search_description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've verified that we have the correct corpus/dataset, let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = tdm_client.get_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a helper function that can standardize and clean up the tokens in it. The function will:\n",
    "\n",
    "* Change all tokens (aka words) to lower case.  This will make 'Cats' and 'cats' be counted as the same token.\n",
    "* Discard tokens with non-alphabetical characters\n",
    "* Discard tokens less than 4 characters in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question to ponder:* Why do you think we want to discard tokens that are less than 4 characters long?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #defines a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #changes all strings to lower case\n",
    "    if len(token) < 4: #discards any tokens that are less than 4 characters long\n",
    "        return\n",
    "    if not(token.isalpha()): #discards any tokens with non-alphabetic characters\n",
    "        return\n",
    "    return token #returns the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the corpus with our helper function.  This may take a while to run; recall that if it's in process, you'll see this: In [ * ]. (No output will show.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tdm_client.dataset_reader(dataset_file)\n",
    "\n",
    "#Creates a new variable `documents` that is a list that that will contain all of our documents.\n",
    "documents = []\n",
    "\n",
    "for n, document in enumerate(reader):\n",
    "    this_doc = []\n",
    "    _id = document[\"id\"]\n",
    "    for token, count in document[\"unigramCount\"].items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append((_id, this_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([d[1] for d in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in [d[1] for d in documents]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have those pieces in place, we can run the following code cells to find the most significant terms, by TFIDF, in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, weight in sorted_td[:20]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 20 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(documents[n][0], dictionary.get(word_id), score)\n",
    "    if n >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional:  How would you print the most significant word for the **first 8 documents**? Copy the code block above, and paste then modify the code in the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to learn more and/or try setting up your own Jupyter Notebook?   [This is a great tutorial.](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
