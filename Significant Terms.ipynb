{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining to do on this: tweak some of the model language, dumb it down/re-explain e.g. what's a token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Teaching note*\n",
    "* *Run from the menu: Cell, All outputs, Clear at the beginning to initialize all cell inputs to the blank state*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Start here](https://hub.tdm-pilot.org/user/jfergusonnortheasternedu/notebooks/jasfTest/About%20the%20Corpus%20and%20About%20Jupyter.ipynb) with the 'About the Corpus/About Jupyter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding significant words within a curated dataset##\n",
    "\n",
    "This notebook demonstrates how to find the significant words in your dataset using a model called TF-IDF. As you work through this notebook you'll take the following steps:\n",
    "\n",
    "* Import your dataset\n",
    "* Find your initial query within your dataset's metadata\n",
    "* Write a helper function to help clean up a single token\n",
    "* Clean each document of your dataset, one token at a time\n",
    "* Use a dictionary of English words to remove words with poor OCR\n",
    "* Compute the most significant words in your corpus using TFIDF and a library callled gensim \n",
    "\n",
    "**What's a token?** It's a string of text. For our purposes, think of a token = a single word.\n",
    "\n",
    "First, we import the Dataset module from the tdm_client library. The tdm_client library contains functions for connecting to the JSTOR server containing our corpus dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from tdm_client import Dataset\n",
    "from tdm_client import htrc_corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze your dataset, use the [dataset ID](./key-terms.ipynb/#dataset-ID) provided when you created your [dataset](./key-terms.ipynb/#dataset). A copy of your [dataset ID](./key-terms.ipynb/#dataset-ID) was sent to your email when you created your [corpus](./key-terms.ipynb#corpus). It should look like a long series of characters surrounded by dashes.  \n",
    "\n",
    "We create a new variable **dset** and initialize its value using the **Dataset** function. A sample **dataset ID** of data derived from searching JSTOR for 'antibiotic' and 'resistance' and 'coli' is provided here ('730b508b-5152-618a-2856-aa1a2900a0b2'). Pasting your unique **dataset ID** here will import your dataset from the JSTOR server.\n",
    "\n",
    "**Note**: If you are curious what is in your dataset, there is a download link in the email you received. The format and content of the files is described in the notebook [Building a Dataset](./1-building-a-dataset.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset('730b508b-5152-618a-2856-aa1a2900a0b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8393"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check this to make sure we have the correct dataset. We can look at the original query by using the query_text method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"antibiotic coli resistance\" from JSTOR from 1985 - 2020'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've verified that we have the correct corpus, let's create a helper function that can standardize and clean up the tokens in our dataset. The function will:\n",
    "\n",
    "* Change all tokens (aka words) to lower case.  This will make 'Meat' and 'meat' be counted as the same token.\n",
    "* Use a dictionary from The HathiTrust Research Center to correct common OCR (Optical Character Recognition) problems\n",
    "* Discard tokens less than 4 characters in length. *(Why?)*\n",
    "* Discard tokens with non-alphabetical characters\n",
    "* Remove stopwords based on an The HathiTrust Research Center stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token): #defines a function `process_token` that takes the argument `token`\n",
    "    token = token.lower() #changes all strings to lower case\n",
    "    corrected = htrc_corrections.get(token) #this is a function that fixes common OCR errors\n",
    "    if corrected is not None: #if corrected has a value, set the `token` variable to the same value as `corrected`\n",
    "        token = corrected\n",
    "    if len(token) < 4: #discards any tokens that are less than 4 characters long\n",
    "        return\n",
    "    if not(token.isalpha()): #discards any tokens with non-alphabetic characters\n",
    "        return\n",
    "    return token #returns the `token` variable which has been set equal to the `corrected` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cycle through each document in the [corpus](./key-terms.ipynb#corpus) with our helper function.  This may take a while to run; recall that if it's in process, you'll see an * in the In [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [] #Creates a new variable `documents` that is a list that will contain all of our documents.\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    documents.append(this_doc)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most significant terms, by TFIDF, in the curated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = {\n",
    "        dictionary.get(_id): value for doc in corpus_tfidf\n",
    "        for _id, value in doc\n",
    "    }\n",
    "sorted_td = sorted(td.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creg 0.9841662944870286\n",
      "mdrgnb 0.975557848630102\n",
      "biba 0.9715613436948968\n",
      "orndcase 0.9631345518081613\n",
      "gapr 0.9529489230155735\n",
      "annatl 0.945784372964842\n",
      "whmd 0.9408637418840726\n",
      "npma 0.9357097608681134\n",
      "mprl 0.9350557470086153\n",
      "samp 0.9334536614930616\n",
      "mcjd 0.9311354394764306\n",
      "lemir 0.9301171834163956\n",
      "tsll 0.9286491029905937\n",
      "cpkp 0.9279376596706661\n",
      "geomorphus 0.924834020485281\n",
      "becs 0.9233882319798145\n",
      "bmta 0.9209127411481993\n",
      "myotubularin 0.9203318325054199\n",
      "hvraf 0.9194049619562112\n",
      "crdof 0.9182367837830574\n"
     ]
    }
   ],
   "source": [
    "for term, weight in sorted_td[:20]:\n",
    "    print(term, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the most significant word, by TFIDF, for the first 20 documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/30142230 mycophagy 0.4446299264467046\n",
      "http://www.jstor.org/stable/30224786 cimfl 0.6369482204030475\n",
      "http://www.jstor.org/stable/4091495 caerulescens 0.5779491895296922\n",
      "http://www.jstor.org/stable/1514055 biocontrol 0.32200863535477997\n",
      "http://www.jstor.org/stable/1514056 elsas 0.38096349437582494\n",
      "http://www.jstor.org/stable/newphytologist.196.2.561 lotus 0.5725634239462112\n",
      "http://www.jstor.org/stable/newphytologist.202.4.1142 phytologist 0.46315267805695803\n",
      "http://www.jstor.org/stable/20869142 arsenic 0.6492155880829461\n",
      "http://www.jstor.org/stable/2558634 rhizobium 0.32451365751263717\n",
      "http://www.jstor.org/stable/2558646 leguminosarum 0.3690918330439759\n",
      "http://www.jstor.org/stable/2558647 biocontrol 0.4795072654959623\n",
      "http://www.jstor.org/stable/newphytologist.200.3.847 flic 0.44618239623871403\n",
      "http://www.jstor.org/stable/40864570 climate 0.7830237862013699\n",
      "http://www.jstor.org/stable/44614859 flight 0.6111188059818026\n",
      "http://www.jstor.org/stable/44471701 vcds 0.7591267678500386\n",
      "http://www.jstor.org/stable/44740046 astronauts 0.33145421982290885\n",
      "http://www.jstor.org/stable/44740008 corrosion 0.4666699462996607\n",
      "http://www.jstor.org/stable/20166554 exemption 0.23161765733898973\n",
      "http://www.jstor.org/stable/3577713 transfected 0.3918464769024746\n",
      "http://www.jstor.org/stable/41061231 injury 0.5325863038517494\n",
      "http://www.jstor.org/stable/30133241 handfeeding 0.3544754638018896\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional (easy):  How would you print the most significant word for the first 8 documents? Modify the code block above and paste your modified code in the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.jstor.org/stable/30142230 mycophagy 0.4446299264467046\n",
      "http://www.jstor.org/stable/30224786 cimfl 0.6369482204030475\n",
      "http://www.jstor.org/stable/4091495 caerulescens 0.5779491895296922\n",
      "http://www.jstor.org/stable/1514055 biocontrol 0.32200863535477997\n",
      "http://www.jstor.org/stable/1514056 elsas 0.38096349437582494\n",
      "http://www.jstor.org/stable/newphytologist.196.2.561 lotus 0.5725634239462112\n",
      "http://www.jstor.org/stable/newphytologist.202.4.1142 phytologist 0.46315267805695803\n",
      "http://www.jstor.org/stable/20869142 arsenic 0.6492155880829461\n",
      "http://www.jstor.org/stable/2558634 rhizobium 0.32451365751263717\n"
     ]
    }
   ],
   "source": [
    "for n, doc in enumerate(corpus_tfidf):\n",
    "    if len(doc) < 1:\n",
    "        continue\n",
    "    word_id, score = max(doc, key=lambda x: x[1])\n",
    "    print(dset.items[n], dictionary.get(word_id), score)\n",
    "    if n >= 8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to learn more and/or try setting up your own Jupyter Notebook?   [This is a great tutorial.](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
